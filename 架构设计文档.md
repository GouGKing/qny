# AI角色扮演对话系统架构设计文档

## 1. 项目概述

AI角色扮演对话系统是一个集成了语音识别、文本生成和语音合成功能的交互式应用，允许用户与不同角色进行文本或语音形式的对话。系统支持多种语言模型切换和实时对话功能，为用户提供沉浸式的AI交互体验。

## 2. 系统架构

### 2.1 整体架构

系统采用前后端分离的架构模式，主要由以下几部分组成：
- 前端：React + Vite构建的单页应用
- 后端：Node.js + Express提供的REST API和WebSocket服务
- 数据库：SQLite存储角色信息
- 语音处理：集成whisper.cpp (STT) 和 piper-tts (TTS)
- LLM集成：支持本地Ollama模型和远程DeepSeek模型

### 2.2 数据流

1. 用户通过前端界面选择角色和交互方式（文本/语音）
2. 输入内容通过WebSocket或REST API发送到后端
3. whisper.cpp进行语音转文字处理，调用LLM生成回复
4. 生成的回复通过文本转语音（TTS）转换为语音
5. 回复文本和音频通过WebSocket或REST API返回前端展示

## 3. 模块规格

### 3.1 前端模块

#### 3.1.1 主界面模块 (App.jsx)

**功能**: 应用的主入口，负责展示角色列表和交互界面

**组件结构**: 
```
App
├── 角色选择区
├── 交互输入区
│   ├── 文本输入框
│   ├── 语音输入按钮
│   └── 模型选择器
└── 实时对话入口
    ├── 实时对话
    └── AI面试
```

**主要功能点**: 
- 角色列表加载与展示
- 角色搜索功能
- 文本和语音输入处理
- 模型切换功能
- WebSocket连接管理
- 音频播放控制

**关键API**: 
- `fetch("http://localhost:3000/api/roles")`: 获取角色列表
- WebSocket消息处理: 接收和发送聊天消息

#### 3.1.2 实时对话模块 (RealtimeChat.jsx)(前端已完善，服务未完善)

**功能**: 提供实时语音对话界面和面试模式

**组件结构**: 
```
RealtimeChat
├── 控制面板
│   ├── 录制按钮
│   ├── 静音按钮
│   ├── 免提按钮
│   └── 通话计时器
└── 面试模式控制
```

**主要功能点**: 
- 实时音频流录制和传输
- 通话时长计时
- 静音和免提控制
- 面试模式支持
- 面试问题管理

**关键API**: 
- `MediaRecorder`: 浏览器音频录制API
- WebSocket消息处理: 实时音频块传输

### 3.2 后端模块

#### 3.2.1 Web服务器模块 (Express)

**功能**: 提供REST API和静态资源服务

**主要功能点**: 
- 提供角色管理API
- 处理文本和语音聊天请求
- 提供前端静态文件服务
- 支持SPA路由重定向

**关键API端点**: 
- `GET /api/roles`: 获取所有角色信息
- `POST /api/chat`: 处理文本聊天请求
- `POST /api/voice-chat`: 处理语音聊天请求

#### 3.2.2 WebSocket服务模块

**功能**: 提供双向实时通信，支持实时对话功能

**主要功能点**: 
- 管理客户端WebSocket连接
- 处理文本消息、语音数据块和控制指令
- 支持面试模式的特殊指令处理
- 提供错误处理和状态管理

**关键消息类型**: 
- `text`: 纯文本消息
- `audio-chunk`: 音频数据块
- `stop`: 停止音频传输
- `config`: 角色配置
- `pause`/`resume`: 音频播放控制
- `regenerate`: 重新生成回复

#### 3.2.3 语言模型集成模块

**功能**: 集成多种LLM模型，提供统一的调用接口

**主要组件**: 
- `chatWithLLM`: 统一的LLM调用入口
- `chatWithOllama`: 本地Ollama模型调用
- `chatWithDeepSeek`: 远程DeepSeek模型调用

**支持的模型**: 
- 本地模型: Mistral (通过Ollama运行)
- 远程模型: DeepSeek (通过API调用)

#### 3.2.4 语音处理模块

**功能**: 负责语音识别(STT)和语音合成(TTS)

**主要组件**: 
- `transcribeBuffer`: 音频缓冲区转文字
- `transcribeAudio`: 音频文件转文字
- `synthesizeSpeech`: 文本转语音
- `generateBackupBeep`: TTS失败时的备用提示音

**核心依赖**: 
- whisper.cpp: 用于语音识别
- ffmpeg: 用于音频格式转换
- piper-tts (Python): 用于语音合成

#### 3.2.5 数据库模块

**功能**: 存储和管理角色信息

**数据结构**: 
```sql
CREATE TABLE roles (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  name TEXT NOT NULL,
  system_prompt TEXT NOT NULL,
  voice_model TEXT NOT NULL,
  feature1 TEXT NOT NULL,
  feature2 TEXT NOT NULL,
  feature3 TEXT NOT NULL
);
```

**主要功能点**: 
- 角色信息增删改查
- 角色特征点管理
- 语音模型关联

## 4. 技术栈

### 4.1 前端技术栈
- React: UI组件库
- Vite: 构建工具
- WebSocket API: 实时通信
- MediaRecorder API: 音频录制

### 4.2 后端技术栈
- Node.js: 运行环境
- Express: Web框架
- WebSocket: 实时通信
- SQLite (better-sqlite3): 数据库
- multer: 文件上传
- child_process: 调用外部程序

### 4.3 语音处理技术
- whisper.cpp: 语音识别
- piper-tts (Python): 语音合成
- ffmpeg: 音频格式转换

### 4.4 AI模型
- Ollama + Mistral: 本地LLM模型
- DeepSeek API: 远程LLM模型

## 5. 个人开发计划

### 5.1 个人分工

| 角色 | 职责 | 技术栈 |
| ---- | ---- | ------ |
| 前端开发 | 开发用户界面和交互功能 | React, Vite, WebSocket API |
| 后端开发 | 开发服务器和API | Node.js, Express, WebSocket |
| AI集成工程师 | 集成LLM模型和语音处理功能 | whisper.cpp, piper-tts, Ollama API |
| 数据库管理员 | 设计和维护数据库 | SQLite |

### 5.2 模块开发分工

| 模块 | 负责人 | 主要任务 |
| ---- | ------ | -------- |
| 前端主界面 | 前端开发 | 角色展示、聊天界面、模型选择器 |
| 实时对话界面 | 前端开发 | 实时对话UI、面试模式 |
| REST API | 后端开发 | 角色管理、文本/语音聊天接口 |
| WebSocket服务 | 后端开发 | 实时通信、消息处理 |
| LLM集成 | AI集成工程师 | 模型接入、统一调用接口 |
| 语音处理 | AI集成工程师 | STT/TTS功能实现、音频格式转换 |
| 数据库 | 数据库管理员 | 数据结构设计、CRUD操作 |